# Message Service

Microservicio especializado en el procesamiento, almacenamiento y gesti√≥n de mensajes individuales dentro del ecosistema Character Chat API.

## üìã Responsabilidades del Servicio

### Responsabilidades Principales
- **Procesamiento de Mensajes**: Validaci√≥n, sanitizaci√≥n y enriquecimiento de contenido
- **Almacenamiento Escalable**: Persistencia eficiente de grandes vol√∫menes de mensajes
- **Integraci√≥n LLM**: Orquestaci√≥n de llamadas a servicios de IA (LM Studio, OpenAI, etc.)
- **Context Management**: Gesti√≥n de ventanas de contexto para conversaciones
- **Token Tracking**: Monitoreo y an√°lisis de uso de tokens
- **Message Analytics**: Estad√≠sticas y m√©tricas de uso de mensajes
- **Content Filtering**: Filtrado y moderaci√≥n de contenido

### Responsabilidades Secundarias
- **Message Search**: B√∫squeda de mensajes por contenido y metadatos
- **Message Export**: Exportaci√≥n de historial de mensajes
- **Rate Limiting**: Control de frecuencia de env√≠o de mensajes
- **Audit Logging**: Registro de actividad para auditor√≠a
- **Message Templates**: Gesti√≥n de plantillas de mensajes predefinidas

## üö´ Limitaciones del Servicio

### Limitaciones Arquitecturales
- **No gestiona conversaciones**: Las conversaciones son responsabilidad del Conversation Service
- **No maneja autenticaci√≥n**: Depende del Auth Service para validaci√≥n de usuarios
- **No gestiona personajes**: Los personajes son responsabilidad del Characters Service
- **No maneja configuraciones**: Las configuraciones de conversaci√≥n est√°n en Conversation Service

### Limitaciones T√©cnicas
- **Volumen de mensajes**: M√°ximo 10,000 mensajes por conversaci√≥n
- **Tama√±o de mensaje**: M√°ximo 50,000 caracteres por mensaje
- **Retenci√≥n de datos**: 2 a√±os para mensajes activos, 6 meses para archivados
- **Rate limiting**: 100 mensajes por minuto por usuario
- **Context window**: M√°ximo 32,000 tokens por contexto
- **Concurrent processing**: M√°ximo 1000 mensajes concurrentes

### Limitaciones Funcionales
- **No modifica mensajes**: Los mensajes son inmutables una vez creados
- **No elimina mensajes**: Solo archivado l√≥gico (soft delete)
- **No gestiona hilos**: Conversaciones lineales √∫nicamente
- **No maneja archivos**: Solo texto plano y metadatos estructurados

## üèóÔ∏è Arquitectura del Servicio

### Stack Tecnol√≥gico
- **Framework**: FastAPI 0.104+
- **Base de datos**: MongoDB con particionamiento por conversaci√≥n
- **Cache**: Redis para context windows y rate limiting
- **Queue**: Redis Streams para procesamiento as√≠ncrono
- **LLM Integration**: Cliente unificado para m√∫ltiples proveedores
- **Monitoring**: Prometheus + Grafana

### Puerto del Servicio
- **Puerto**: 8004 (Message Service)

## üóÑÔ∏è Estrategia de Almacenamiento

### Elecci√≥n de Base de Datos: MongoDB

**Razones para MongoDB:**
- **Flexibilidad de esquema**: Los mensajes pueden tener metadatos variables
- **Escalabilidad horizontal**: Sharding por conversation_id
- **Performance**: Consultas r√°pidas con √≠ndices optimizados
- **Agregaciones**: Analytics complejas con aggregation pipelines
- **TTL**: Expiraci√≥n autom√°tica de mensajes antiguos

### Alternativas Consideradas y Descartadas

**PostgreSQL con JSONB:**
- ‚ùå **Menos flexible** para esquemas evolutivos
- ‚ùå **Sharding complejo** comparado con MongoDB
- ‚úÖ **ACID transactions** (no cr√≠ticas para mensajes)
- ‚ùå **Agregaciones menos eficientes** que MongoDB

**Cassandra:**
- ‚úÖ **Excelente escalabilidad** para writes
- ‚ùå **Complejidad operacional** alta
- ‚ùå **Consultas limitadas** sin agregaciones complejas
- ‚ùå **Overkill** para el volumen actual

**ElasticSearch:**
- ‚úÖ **Excelente para b√∫squeda** de texto
- ‚ùå **No es base de datos primaria** 
- ‚ùå **Overhead** para operaciones CRUD simples
- ‚úÖ **Se usar√° como √≠ndice secundario**

## üìä Modelos de Datos

### Colecci√≥n Principal: messages

```javascript
{
  "_id": ObjectId("507f1f77bcf86cd799439011"),
  "message_id": "msg_abc123def456",           // UUID √∫nico
  "conversation_id": "conv_789xyz123",        // Foreign key a Conversation Service
  "user_id": "user_456def789",               // Foreign key a Auth Service
  "character_id": "char_123abc456",          // Foreign key a Characters Service
  
  "content": {
    "text": "Hello Luna! How are you today?",
    "sanitized_text": "Hello Luna! How are you today?",
    "detected_language": "en",
    "content_hash": "sha256:abc123...",       // Para deduplicaci√≥n
    "word_count": 6,
    "character_count": 30
  },
  
  "role": "user",                            // user | assistant | system
  "message_type": "standard",                // standard | template | system
  "status": "active",                        // active | archived | flagged
  
  "llm_metadata": {
    "provider": "lmstudio",                  // lmstudio | openai | anthropic
    "model": "google/gemma-3-12b",
    "temperature": 0.7,
    "max_tokens": 2048,
    "stop_sequences": ["\\n\\nHuman:"],
    "request_id": "req_xyz789abc123"
  },
  
  "token_usage": {
    "prompt_tokens": 245,
    "completion_tokens": 67,
    "total_tokens": 312,
    "estimated_cost": 0.00234               // USD
  },
  
  "context_metadata": {
    "context_window_size": 20,
    "messages_in_context": 15,
    "context_tokens": 2890,
    "context_hash": "sha256:def456..."      // Para cache
  },
  
  "processing_metadata": {
    "processing_time_ms": 1234,
    "queue_time_ms": 45,
    "total_latency_ms": 1279,
    "retry_count": 0,
    "error_count": 0
  },
  
  "safety_metadata": {
    "content_filtered": false,
    "safety_score": 0.95,                   // 0-1 (1 = safe)
    "detected_issues": [],
    "moderation_flags": []
  },
  
  "timestamps": {
    "created_at": ISODate("2025-01-15T10:30:00.123Z"),
    "processed_at": ISODate("2025-01-15T10:30:01.456Z"),
    "llm_response_at": ISODate("2025-01-15T10:30:02.789Z")
  },
  
  "client_metadata": {
    "client_msg_id": "client_msg_123",      // Para idempotencia
    "user_agent": "CharacterChat/1.0",
    "ip_address": "192.168.1.100",         // Para rate limiting
    "platform": "web"                      // web | mobile | api
  },
  
  "custom_metadata": {                      // Metadatos extensibles
    "intent": "greeting",
    "sentiment": "positive",
    "topics": ["greeting", "wellbeing"],
    "urgency": "low"
  },
  
  "version": 1,                            // Para versionado de esquema
  "partition_key": "conv_789xyz123"        // Para sharding
}
```

### √çndices de MongoDB

```javascript
// √çndice primario
db.messages.createIndex({ "_id": 1 })

// √çndices para consultas frecuentes
db.messages.createIndex({ 
  "conversation_id": 1, 
  "timestamps.created_at": -1 
})

db.messages.createIndex({ 
  "user_id": 1, 
  "timestamps.created_at": -1 
})

db.messages.createIndex({ 
  "message_id": 1 
}, { unique: true })

// √çndices para b√∫squeda
db.messages.createIndex({ 
  "content.text": "text",
  "custom_metadata.topics": "text"
})

// √çndices para analytics
db.messages.createIndex({ 
  "llm_metadata.provider": 1,
  "llm_metadata.model": 1,
  "timestamps.created_at": -1
})

db.messages.createIndex({ 
  "token_usage.total_tokens": 1,
  "timestamps.created_at": -1
})

// √çndice TTL para limpieza autom√°tica
db.messages.createIndex(
  { "timestamps.created_at": 1 },
  { 
    expireAfterSeconds: 63072000,  // 2 a√±os
    partialFilterExpression: { "status": "archived" }
  }
)

// √çndice para sharding
db.messages.createIndex({ 
  "partition_key": "hashed" 
})
```

## üì° API Endpoints

### Gesti√≥n de Mensajes

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| `POST` | `/messages` | Enviar nuevo mensaje |
| `GET` | `/messages/{message_id}` | Obtener mensaje espec√≠fico |
| `GET` | `/conversations/{conv_id}/messages` | Listar mensajes de conversaci√≥n |
| `PATCH` | `/messages/{message_id}/metadata` | Actualizar metadatos |
| `DELETE` | `/messages/{message_id}` | Archivar mensaje |
| `POST` | `/messages/{message_id}/restore` | Restaurar mensaje archivado |

### Procesamiento LLM

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| `POST` | `/llm/process` | Procesar mensaje con LLM |
| `GET` | `/llm/providers` | Listar proveedores disponibles |
| `GET` | `/llm/models` | Listar modelos disponibles |
| `POST` | `/llm/validate` | Validar configuraci√≥n LLM |

### Analytics y Estad√≠sticas

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| `GET` | `/conversations/{conv_id}/stats` | Estad√≠sticas de conversaci√≥n |
| `GET` | `/users/{user_id}/message-stats` | Estad√≠sticas de usuario |
| `GET` | `/analytics/token-usage` | An√°lisis de uso de tokens |
| `GET` | `/analytics/performance` | M√©tricas de rendimiento |

### B√∫squeda y Filtrado

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| `GET` | `/search/messages` | B√∫squeda de mensajes |
| `POST` | `/search/advanced` | B√∫squeda avanzada |
| `GET` | `/search/suggestions` | Sugerencias de b√∫squeda |

### Administraci√≥n

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| `GET` | `/health` | Health check del servicio |
| `GET` | `/metrics` | M√©tricas de Prometheus |
| `POST` | `/admin/cleanup` | Limpieza de mensajes antiguos |
| `GET` | `/admin/queue-status` | Estado de colas de procesamiento |

## üîÑ Flujos de Trabajo Principales

### 1. Env√≠o de Mensaje

```
1. Cliente ‚Üí POST /messages
2. Validaci√≥n y sanitizaci√≥n de contenido
3. Rate limiting check
4. Encolado para procesamiento as√≠ncrono
5. Respuesta inmediata con message_id
6. Procesamiento en background:
   - Obtener contexto de conversaci√≥n
   - Llamada a LLM Service
   - Generaci√≥n de respuesta
   - Almacenamiento de ambos mensajes
   - Notificaci√≥n de completado
```

### 2. Obtenci√≥n de Mensajes

```
1. Cliente ‚Üí GET /conversations/{id}/messages
2. Verificaci√≥n de autorizaci√≥n con Auth Service
3. Consulta optimizada a MongoDB
4. Aplicaci√≥n de filtros y paginaci√≥n
5. Enriquecimiento con metadatos
6. Respuesta con mensajes paginados
```

### 3. Procesamiento LLM

```
1. Mensaje encolado ‚Üí Redis Stream
2. Worker obtiene mensaje de cola
3. Construcci√≥n de contexto:
   - Obtener mensajes recientes
   - Aplicar context window
   - Obtener configuraci√≥n de personaje
4. Llamada a LLM Provider
5. Post-procesamiento de respuesta
6. Almacenamiento con metadatos completos
7. Actualizaci√≥n de m√©tricas
```

## üîß Configuraci√≥n del Servicio

### Variables de Entorno

```bash
# Service Configuration
SERVICE_NAME=message-service
PORT=8004
LOG_LEVEL=INFO
ENVIRONMENT=production

# Database
MONGODB_URL=mongodb://mongodb-cluster:27017
MONGODB_DATABASE=message_service
MONGODB_CONNECTION_POOL_MIN=20
MONGODB_CONNECTION_POOL_MAX=100

# Redis (Cache & Queue)
REDIS_URL=redis://redis-cluster:6379
REDIS_CONNECTION_POOL_MAX=50
CACHE_TTL_SECONDS=3600

# External Services
AUTH_SERVICE_URL=http://auth-service:8001
CONVERSATION_SERVICE_URL=http://conversation-service:8003
CHARACTERS_SERVICE_URL=http://characters-service:8002
LLM_SERVICE_URL=http://llm-service:8005

# LLM Configuration
DEFAULT_LLM_PROVIDER=lmstudio
DEFAULT_MODEL=google/gemma-3-12b
MAX_TOKENS_PER_REQUEST=4096
DEFAULT_TEMPERATURE=0.7
REQUEST_TIMEOUT_SECONDS=60

# Rate Limiting
MAX_MESSAGES_PER_MINUTE=100
MAX_MESSAGES_PER_HOUR=1000
MAX_MESSAGES_PER_DAY=10000

# Content Safety
ENABLE_CONTENT_FILTERING=true
SAFETY_THRESHOLD=0.8
MAX_MESSAGE_LENGTH=50000

# Performance
MAX_CONCURRENT_LLM_REQUESTS=100
MESSAGE_PROCESSING_TIMEOUT=300
BATCH_SIZE_FOR_ANALYTICS=1000

# Data Retention
MESSAGE_RETENTION_DAYS=730
ARCHIVED_MESSAGE_RETENTION_DAYS=180
AUTO_CLEANUP_ENABLED=true
```

## üèõÔ∏è Estructura del Proyecto

```
message-service/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                        # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ config.py                      # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ database.py                    # MongoDB connection
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/                          # API Layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py           # FastAPI dependencies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middleware.py             # Custom middleware
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v1/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ messages.py           # Message endpoints
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ llm.py               # LLM processing endpoints
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ analytics.py         # Analytics endpoints
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ search.py            # Search endpoints
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ admin.py             # Admin endpoints
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/                     # Service Layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ message_service.py        # Core message business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py           # LLM integration service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content_service.py       # Content processing & filtering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics_service.py     # Analytics and metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search_service.py        # Search functionality
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ external_services.py     # External service clients
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ repositories/                 # Repository Layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ message_repository.py    # Message data access
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics_repository.py  # Analytics queries
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search_repository.py     # Search operations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base_repository.py       # Abstract repository
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                       # Data Models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ message.py               # Message Pydantic models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm.py                   # LLM related models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics.py             # Analytics models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.py                # Search models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requests.py              # API request models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ responses.py             # API response models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py              # Beanie MongoDB models
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ workers/                      # Background Workers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ message_processor.py     # Async message processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_worker.py           # LLM processing worker
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics_worker.py     # Analytics computation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cleanup_worker.py       # Data cleanup tasks
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ core/                        # Core Utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py            # Custom exceptions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security.py              # Security utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache.py                 # Redis cache wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queue.py                 # Redis queue wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging.py               # Logging configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py               # Prometheus metrics
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rate_limiter.py          # Rate limiting logic
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                       # General Utilities
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ content_utils.py         # Content processing helpers
‚îÇ       ‚îú‚îÄ‚îÄ token_utils.py           # Token counting utilities
‚îÇ       ‚îú‚îÄ‚îÄ datetime_utils.py        # Date/time helpers
‚îÇ       ‚îú‚îÄ‚îÄ pagination.py            # Pagination utilities
‚îÇ       ‚îú‚îÄ‚îÄ validators.py            # Custom validators
‚îÇ       ‚îî‚îÄ‚îÄ text_processing.py       # Text analysis utilities
‚îÇ
‚îú‚îÄ‚îÄ tests/                           # Test Suite
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                  # Pytest configuration
‚îÇ   ‚îú‚îÄ‚îÄ unit/                        # Unit tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_repositories/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_utils/
‚îÇ   ‚îú‚îÄ‚îÄ integration/                 # Integration tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_database/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_external_services/
‚îÇ   ‚îî‚îÄ‚îÄ e2e/                        # End-to-end tests
‚îÇ       ‚îî‚îÄ‚îÄ test_message_workflows/
‚îÇ
‚îú‚îÄ‚îÄ scripts/                         # Utility Scripts
‚îÇ   ‚îú‚îÄ‚îÄ migrate_messages.py         # Data migration
‚îÇ   ‚îú‚îÄ‚îÄ setup_indexes.py            # Database index setup
‚îÇ   ‚îú‚îÄ‚îÄ cleanup_old_messages.py     # Data cleanup
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_performance.py    # Performance testing
‚îÇ   ‚îî‚îÄ‚îÄ seed_test_data.py           # Test data generation
‚îÇ
‚îú‚îÄ‚îÄ workers/                         # Worker Entry Points
‚îÇ   ‚îú‚îÄ‚îÄ message_processor.py        # Message processing worker
‚îÇ   ‚îú‚îÄ‚îÄ llm_worker.py              # LLM processing worker
‚îÇ   ‚îú‚îÄ‚îÄ analytics_worker.py        # Analytics worker
‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py               # Task scheduler
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ requirements-dev.txt             # Development dependencies
‚îú‚îÄ‚îÄ requirements-workers.txt         # Worker-specific dependencies
‚îú‚îÄ‚îÄ Dockerfile                       # Container definition
‚îú‚îÄ‚îÄ Dockerfile.worker                # Worker container definition
‚îú‚îÄ‚îÄ docker-compose.yml               # Local development setup
‚îú‚îÄ‚îÄ .env.example                     # Environment template
‚îú‚îÄ‚îÄ pyproject.toml                   # Python project configuration
‚îî‚îÄ‚îÄ monitoring/                      # Monitoring configuration
    ‚îú‚îÄ‚îÄ prometheus.yml
    ‚îî‚îÄ‚îÄ grafana-dashboard.json
```

## üîß Desarrollo Local

### Quick Start

```bash
# 1. Navegar al directorio
cd microservices/message-service

# 2. Crear entorno virtual
python -m venv .venv
source .venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt
pip install -r requirements-dev.txt

# 4. Configurar variables de entorno
cp .env.example .env
# Editar .env con configuraciones locales

# 5. Iniciar servicios de dependencias
docker-compose up -d mongodb redis

# 6. Configurar base de datos
python scripts/setup_indexes.py

# 7. Iniciar el servicio
python -m app.main

# 8. Iniciar workers (en terminal separado)
python workers/message_processor.py
```

### Docker Development

```bash
# Ejecutar todo el stack
docker-compose up -d

# Ver logs
docker-compose logs -f message-service

# Ejecutar tests
docker-compose exec message-service pytest
```

## üìä Monitoreo y M√©tricas

### M√©tricas Principales

```python
# M√©tricas de mensajes
messages_created_total = Counter('messages_created_total', 'Total messages created')
messages_processed_total = Counter('messages_processed_total', 'Total messages processed')
message_processing_time = Histogram('message_processing_seconds', 'Message processing time')
message_queue_size = Gauge('message_queue_size', 'Current message queue size')

# M√©tricas de LLM
llm_requests_total = Counter('llm_requests_total', 'Total LLM requests', ['provider', 'model'])
llm_response_time = Histogram('llm_response_seconds', 'LLM response time', ['provider'])
llm_tokens_used = Counter('llm_tokens_used_total', 'Total tokens used', ['type'])
llm_errors_total = Counter('llm_errors_total', 'Total LLM errors', ['provider', 'error_type'])

# M√©tricas de rendimiento
database_query_time = Histogram('database_query_seconds', 'Database query time', ['operation'])
cache_hit_rate = Gauge('cache_hit_rate', 'Cache hit rate percentage')
concurrent_requests = Gauge('concurrent_requests', 'Current concurrent requests')
```

### Health Checks

```python
# Endpoint: GET /health
{
  "status": "healthy",
  "service": "message-service",
  "version": "1.0.0",
  "timestamp": "2025-01-15T17:00:00.000Z",
  "dependencies": {
    "mongodb": {
      "status": "connected",
      "response_time_ms": 15,
      "connection_pool": {
        "active": 25,
        "idle": 15,
        "max": 100
      }
    },
    "redis": {
      "status": "connected",
      "response_time_ms": 3,
      "memory_usage": "45MB"
    },
    "llm_service": {
      "status": "reachable",
      "response_time_ms": 120,
      "queue_size": 5
    },
    "auth_service": {
      "status": "reachable",
      "response_time_ms": 25
    }
  },
  "performance": {
    "avg_message_processing_time_ms": 850,
    "messages_in_queue": 12,
    "cache_hit_rate": 0.87,
    "concurrent_requests": 23
  }
}
```

## üõ°Ô∏è Seguridad y Validaci√≥n

### Content Safety

```python
class ContentSafetyService:
    async def validate_message_content(self, content: str) -> SafetyResult:
        safety_checks = [
            self.check_length_limits(content),
            self.check_harmful_content(content),
            self.check_spam_patterns(content),
            self.check_personal_info(content),
            self.check_language_appropriateness(content)
        ]
        
        return await self.aggregate_safety_results(safety_checks)
```

### Rate Limiting

```python
class MessageRateLimiter:
    async def check_user_limits(self, user_id: str) -> RateLimitResult:
        limits = [
            await self.check_per_minute_limit(user_id),
            await self.check_per_hour_limit(user_id),
            await self.check_per_day_limit(user_id),
            await self.check_concurrent_requests(user_id)
        ]
        
        return self.evaluate_limits(limits)
```

## üìà Escalabilidad y Performance

### Sharding Strategy

```javascript
// Sharding por conversation_id para distribuci√≥n uniforme
sh.shardCollection("message_service.messages", {"conversation_id": "hashed"})

// Cada shard maneja un subset de conversaciones
// Consultas por conversaci√≥n son locales a un shard
// Consultas globales requieren scatter-gather
```

### Caching Strategy

```python
# Context Window Cache
cache_key = f"context:{conversation_id}:{window_size}"
context = await redis.get(cache_key)
if not context:
    context = await build_context_window(conversation_id, window_size)
    await redis.setex(cache_key, 300, context)  # 5 minutos TTL

# LLM Response Cache (para mensajes id√©nticos)
content_hash = hashlib.sha256(message_content.encode()).hexdigest()
cache_key = f"llm_response:{content_hash}:{model_config_hash}"
cached_response = await redis.get(cache_key)
```

### Performance Optimizations

```python
# Batch processing para analytics
@background_task(schedule="*/5 * * * *")  # Cada 5 minutos
async def compute_analytics_batch():
    batch = await get_unprocessed_messages(limit=1000)
    analytics = await compute_analytics_parallel(batch)
    await store_analytics_batch(analytics)

# Connection pooling optimizado
mongodb_client = AsyncIOMotorClient(
    connection_string,
    minPoolSize=20,
    maxPoolSize=100,
    maxIdleTimeMS=30000,
    serverSelectionTimeoutMS=5000
)
```

## üö® Troubleshooting

### Problemas Comunes

| Problema | S√≠ntoma | Soluci√≥n |
|----------|---------|----------|
| Cola de mensajes creciendo | Queue size > 1000 | Aumentar workers, verificar LLM service |
| Alta latencia LLM | Response time > 5s | Check LLM service, ajustar timeouts |
| Memory leaks | RAM usage creciendo | Revisar context caching, restart workers |
| Database locks | Slow queries | Optimizar √≠ndices, revisar sharding |
| Rate limit errors | 429 responses | Ajustar l√≠mites, verificar distribuci√≥n |

### Comandos de Diagn√≥stico

```bash
# Verificar estado de colas
curl http://message-service:8004/admin/queue-status

# M√©tricas de performance
curl http://message-service:8004/metrics

# Verificar configuraci√≥n LLM
curl http://message-service:8004/llm/providers

# Test de conectividad
curl http://message-service:8004/health
```

## üìö Documentaci√≥n Adicional

- [API Specification](./API_SPECIFICATION.md) - Documentaci√≥n detallada de endpoints
- [Database Design](./DATABASE_DESIGN.md) - Esquemas y optimizaciones de MongoDB
- [Architecture](./ARCHITECTURE.md) - Patrones y dise√±o del servicio
- [Deployment Guide](./DEPLOYMENT.md) - Gu√≠a completa de deployment
- [LLM Integration Guide](./LLM_INTEGRATION.md) - Integraci√≥n con proveedores LLM
- [Performance Tuning](./PERFORMANCE_TUNING.md) - Optimizaci√≥n y escalabilidad

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Ver archivo LICENSE para m√°s detalles.