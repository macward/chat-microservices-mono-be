# ðŸ¤– ConfiguraciÃ³n de Modelos - Servicio LLM

Esta guÃ­a explica cÃ³mo se asignan y configuran los modelos en el Servicio LLM.

## ðŸ“ DÃ³nde se Asigna el Modelo

El modelo se asigna en **3 lugares principales** del cÃ³digo:

### 1. **ConfiguraciÃ³n por Defecto** (`app/config.py`)

```python
# LÃ­neas 32-35
default_model: str = Field(
    "dirty-muse-writer-v01-uncensored-erotica-nsfw",
    description="Modelo LLM por defecto"
)
```

**PropÃ³sito**: Define quÃ© modelo usar cuando no se especifica uno en la peticiÃ³n.

### 2. **En cada PeticiÃ³n HTTP** (especificado por el cliente)

```json
{
    "model": "dirty-muse-writer-v01-uncensored-erotica-nsfw",
    "messages": [
        {"role": "user", "content": "Â¡Hola!"}
    ]
}
```

**PropÃ³sito**: Permite al cliente especificar quÃ© modelo usar para esa peticiÃ³n especÃ­fica.

### 3. **LÃ³gica de SelecciÃ³n** (`app/services/llm_service.py`)

```python
# LÃ­nea 100
model_to_use = request.model if request.model else settings.default_model
kwargs["model"] = model_to_use
```

**PropÃ³sito**: Si el cliente no especifica modelo, usa el configurado por defecto.

---

## ðŸ”§ CÃ³mo Cambiar el Modelo

### OpciÃ³n 1: Variable de Entorno (Recomendado)

```bash
# En .env
LLM_SERVICE_DEFAULT_MODEL=google/gemma-3-12b
```

### OpciÃ³n 2: Modificar config.py

```python
default_model: str = Field(
    "google/gemma-3-12b",  # Cambiar aquÃ­
    description="Modelo LLM por defecto"
)
```

### OpciÃ³n 3: Especificar en cada PeticiÃ³n

```python
import requests

response = requests.post("http://localhost:8000/llm/message", json={
    "model": "google/gemma-3-12b",  # Modelo especÃ­fico
    "messages": [{"role": "user", "content": "Â¡Hola!"}]
})
```

---

## ðŸ“‹ Modelos Disponibles Actualmente

SegÃºn LM Studio, tienes estos modelos cargados:

```json
{
    "data": [
        {
            "id": "dirty-muse-writer-v01-uncensored-erotica-nsfw",
            "object": "model",
            "owned_by": "organization_owner"
        },
        {
            "id": "dirty-muse-writer-v01-uncensored-erotica-nsfw:2",
            "object": "model", 
            "owned_by": "organization_owner"
        },
        {
            "id": "text-embedding-nomic-embed-text-v1.5",
            "object": "model",
            "owned_by": "organization_owner"
        },
        {
            "id": "google/gemma-3-12b",
            "object": "model",
            "owned_by": "organization_owner"
        }
    ]
}
```

### âœ… **Modelo Configurado Actualmente**
- `dirty-muse-writer-v01-uncensored-erotica-nsfw` (por defecto)

### ðŸ”„ **Modelos Alternativos Disponibles**
- `dirty-muse-writer-v01-uncensored-erotica-nsfw:2`
- `google/gemma-3-12b`
- `text-embedding-nomic-embed-text-v1.5` (para embeddings)

---

## ðŸš€ Verificar Modelos Disponibles

### Desde la Terminal

```bash
# Ver modelos disponibles en LM Studio
curl -s http://localhost:1234/v1/models | python -m json.tool
```

### Desde el CÃ³digo

```python
import requests

def get_available_models():
    response = requests.get("http://localhost:1234/v1/models")
    if response.status_code == 200:
        models = response.json()
        return [model["id"] for model in models["data"]]
    return []

models = get_available_models()
print("Modelos disponibles:", models)
```

### Desde el Servicio LLM

El servicio automÃ¡ticamente verifica modelos disponibles al inicializar y ajusta el modelo por defecto si es necesario:

```python
# En llm_service.py lÃ­neas 51-64
if settings.default_model not in available_models:
    logger.warning(f"Modelo por defecto '{settings.default_model}' no encontrado. Usando '{available_models[0]}'")
    settings.default_model = available_models[0]
```

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Usar Diferentes Modelos segÃºn el Caso de Uso

```python
# Cliente Python con mÃºltiples modelos
class LLMClient:
    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.models = {
            "chat": "dirty-muse-writer-v01-uncensored-erotica-nsfw",
            "reasoning": "google/gemma-3-12b",
            "embeddings": "text-embedding-nomic-embed-text-v1.5"
        }
    
    def chat(self, message):
        return self._send_message(message, self.models["chat"])
    
    def reason(self, question):
        return self._send_message(question, self.models["reasoning"])
    
    def _send_message(self, message, model):
        response = requests.post(f"{self.base_url}/llm/message", json={
            "model": model,
            "messages": [{"role": "user", "content": message}]
        })
        return response.json()["response"]
```

### ValidaciÃ³n de Modelos

El servicio incluye validaciÃ³n automÃ¡tica:

1. **Al iniciar**: Verifica que hay modelos disponibles
2. **En cada peticiÃ³n**: Valida que el modelo solicitado existe
3. **Fallback automÃ¡tico**: Si el modelo por defecto no existe, usa el primero disponible

---

## ðŸš¨ Troubleshooting

### Error: "Multiple models are loaded. Please specify a model"

**Causa**: LM Studio tiene mÃºltiples modelos y no sabe cuÃ¡l usar.

**SoluciÃ³n**:
1. Especificar modelo en cada peticiÃ³n
2. Configurar modelo por defecto vÃ¡lido
3. El servicio ahora maneja esto automÃ¡ticamente

### Error: "Model not found"

**Causa**: El modelo especificado no estÃ¡ cargado en LM Studio.

**SoluciÃ³n**:
1. Verificar modelos disponibles: `curl http://localhost:1234/v1/models`
2. Usar uno de los modelos listados
3. Cargar el modelo deseado en LM Studio

### Error: "No models available"

**Causa**: LM Studio no tiene ningÃºn modelo cargado.

**SoluciÃ³n**:
1. Abrir LM Studio
2. Cargar al menos un modelo
3. Reiniciar el servicio

---

## ðŸ“ Ejemplos PrÃ¡cticos

### Cambiar Modelo por Variable de Entorno

```bash
# Crear .env
echo "LLM_SERVICE_DEFAULT_MODEL=google/gemma-3-12b" > .env

# Reiniciar servicio
uvicorn app.main:app --reload
```

### Usar Modelo EspecÃ­fico en PeticiÃ³n

```bash
curl -X POST "http://localhost:8000/llm/message" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "google/gemma-3-12b",
    "messages": [{"role": "user", "content": "Â¡Hola!"}],
    "temperature": 0.7
  }'
```

### Cliente que Maneja MÃºltiples Modelos

```python
models_config = {
    "creative_writing": "dirty-muse-writer-v01-uncensored-erotica-nsfw",
    "analytical": "google/gemma-3-12b",
    "conversational": "dirty-muse-writer-v01-uncensored-erotica-nsfw:2"
}

def ask_llm(question, task_type="conversational"):
    model = models_config.get(task_type, models_config["conversational"])
    
    response = requests.post("http://localhost:8000/llm/message", json={
        "model": model,
        "messages": [{"role": "user", "content": question}]
    })
    
    return response.json()["response"]

# Uso
creative_response = ask_llm("Escribe un poema", "creative_writing")
analytical_response = ask_llm("Analiza estos datos", "analytical")
```

---

**âœ… ConfiguraciÃ³n actual**: `dirty-muse-writer-v01-uncensored-erotica-nsfw`

**ðŸ”§ Para cambiar**: Modifica la variable de entorno `LLM_SERVICE_DEFAULT_MODEL` o especifica el modelo en cada peticiÃ³n.