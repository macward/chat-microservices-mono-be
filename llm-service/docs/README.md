# ğŸ“š DocumentaciÃ³n - Servicio LLM

Bienvenido a la documentaciÃ³n completa del **Servicio LLM**. Un microservicio REST robusto para integrar modelos de lenguaje grandes en tus aplicaciones.

## ğŸ¯ Â¿QuÃ© es el Servicio LLM?

Un microservicio **production-ready** que proporciona:
- âœ… **API REST simple** para interactuar con LLMs
- âœ… **Manejo robusto de errores** y reintentos automÃ¡ticos
- âœ… **ValidaciÃ³n de entrada** con Pydantic
- âœ… **Logging estructurado** para debugging
- âœ… **Rate limiting** y seguridad incorporada
- âœ… **Health checks** para monitoreo
- âœ… **Tests comprehensivos** (unitarios + integraciÃ³n)

## ğŸ“– GuÃ­as de DocumentaciÃ³n

### ğŸš€ **Para Empezar RÃ¡pido**
- **[Quick Start](QUICK_START.md)** - Â¡De 0 a funcionando en 5 minutos!

### ğŸ”§ **Para Desarrolladores**
- **[GuÃ­a de IntegraciÃ³n](INTEGRATION_GUIDE.md)** - DocumentaciÃ³n completa para integrar en tu proyecto
- **[Ejemplos de API](API_EXAMPLES.md)** - Casos de uso especÃ­ficos con cÃ³digo completo

### ğŸ“‹ **Para Administradores**
- **[README Principal](../README.md)** - ConfiguraciÃ³n, deployment y troubleshooting

## ğŸ¯ Casos de Uso TÃ­picos

### ğŸ¤– **Chatbots y Asistentes**
```python
# Crear un chatbot en 3 lÃ­neas
import requests
response = requests.post("http://localhost:8000/llm/message", json={
    "model": "tu-modelo",
    "messages": [{"role": "user", "content": "Â¡Hola!"}]
})
print(response.json()["response"])
```

### ğŸ“ **GeneraciÃ³n de Contenido**
```python
# Generar un blog post
response = requests.post("http://localhost:8000/llm/message", json={
    "model": "tu-modelo",
    "messages": [{"role": "user", "content": "Escribe un post sobre IA"}],
    "temperature": 0.7,
    "max_tokens": 300
})
```

### ğŸ§  **AnÃ¡lisis de Texto**
```python
# Analizar sentimiento
response = requests.post("http://localhost:8000/llm/message", json={
    "model": "tu-modelo", 
    "messages": [{"role": "user", "content": "Analiza el sentimiento: Me encanta este producto"}]
})
```

### ğŸ’» **Asistente de CÃ³digo**
```python
# Explicar cÃ³digo
code = "def fibonacci(n): return n if n<=1 else fibonacci(n-1)+fibonacci(n-2)"
response = requests.post("http://localhost:8000/llm/message", json={
    "model": "tu-modelo",
    "messages": [{"role": "user", "content": f"Explica este cÃ³digo: {code}"}]
})
```

## ğŸŒŸ CaracterÃ­sticas Principales

### ğŸ”’ **Seguridad y Confiabilidad**
- Rate limiting configurable
- Headers de seguridad automÃ¡ticos
- ValidaciÃ³n estricta de entrada
- Manejo granular de errores
- Timeouts y reintentos automÃ¡ticos

### ğŸ“Š **Observabilidad**
- Logging estructurado en JSON
- Correlation IDs para tracking
- MÃ©tricas de performance
- Health checks para monitoreo

### âš¡ **Performance**
- Connection pooling
- Caching opcional
- Procesamiento asÃ­ncrono
- Optimizaciones de producciÃ³n

### ğŸ§ª **Testing**
- Tests unitarios completos
- Tests de integraciÃ³n
- Tests con LLM real
- Scripts de validaciÃ³n manual

## ğŸ“Š Endpoints Disponibles

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/` | GET | InformaciÃ³n del servicio |
| `/health` | GET | Estado de salud y conectividad |
| `/llm/message` | POST | **Enviar mensaje al LLM** |
| `/docs` | GET | DocumentaciÃ³n automÃ¡tica |

## ğŸš€ Flujo de IntegraciÃ³n TÃ­pico

```mermaid
graph LR
    A[Tu App] --> B[Servicio LLM]
    B --> C[LM Studio]
    C --> D[Modelo LLM]
    D --> C
    C --> B
    B --> A
```

1. **Tu aplicaciÃ³n** envÃ­a una peticiÃ³n HTTP POST a `/llm/message`
2. **Servicio LLM** valida, procesa y reenvÃ­a a LM Studio
3. **LM Studio** ejecuta el modelo y devuelve la respuesta
4. **Servicio LLM** procesa y devuelve la respuesta estructurada

## ğŸ¨ Ejemplos de IntegraciÃ³n

### Python
```python
import requests

def ask_llm(question):
    response = requests.post("http://localhost:8000/llm/message", json={
        "model": "tu-modelo",
        "messages": [{"role": "user", "content": question}]
    })
    return response.json()["response"]

answer = ask_llm("Â¿QuÃ© es machine learning?")
```

### JavaScript (Node.js)
```javascript
const axios = require('axios');

async function askLLM(question) {
    const response = await axios.post('http://localhost:8000/llm/message', {
        model: 'tu-modelo',
        messages: [{role: 'user', content: question}]
    });
    return response.data.response;
}

askLLM('Â¿QuÃ© es JavaScript?').then(console.log);
```

### JavaScript (Frontend)
```javascript
async function askLLM(question) {
    const response = await fetch('http://localhost:8000/llm/message', {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify({
            model: 'tu-modelo',
            messages: [{role: 'user', content: question}]
        })
    });
    const data = await response.json();
    return data.response;
}
```

### cURL
```bash
curl -X POST "http://localhost:8000/llm/message" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tu-modelo",
    "messages": [{"role": "user", "content": "Â¡Hola!"}],
    "temperature": 0.7,
    "max_tokens": 100
  }'
```

## ğŸ› ï¸ ConfiguraciÃ³n RÃ¡pida

### 1. Pre-requisitos
```bash
# Verificar LM Studio
curl http://localhost:1234/v1/models
```

### 2. InstalaciÃ³n
```bash
git clone <repo-url>
cd llm-service
pip install -r requirements.txt
```

### 3. Arrancar Servicio
```bash
uvicorn app.main:app --reload
```

### 4. Verificar
```bash
curl http://localhost:8000/health
```

## ğŸ“ˆ Monitoreo y Debugging

### Health Check
```bash
curl http://localhost:8000/health
# Respuesta:
{
  "status": "healthy",
  "llm_service": "connected", 
  "uptime": 3600,
  "version": "1.0.0"
}
```

### Headers Ãštiles
Cada respuesta incluye:
- `X-Correlation-ID`: Para tracking de peticiones
- `X-Process-Time`: Tiempo de procesamiento
- Headers de seguridad estÃ¡ndar

### Logging
```bash
# Logs estructurados en JSON
{"timestamp": "2024-01-01T12:00:00", "level": "INFO", "correlation_id": "abc-123", "message": "PeticiÃ³n completada"}
```

## ğŸš¨ SoluciÃ³n de Problemas

### Errores Comunes

| Error | Causa | SoluciÃ³n |
|-------|-------|----------|
| Connection refused | Servicio no corriendo | `uvicorn app.main:app --reload` |
| Service unavailable (503) | LM Studio desconectado | Verificar LM Studio + modelo cargado |
| Request timeout (408) | PeticiÃ³n muy larga | Reducir `max_tokens` |
| Rate limit (429) | Demasiadas peticiones | Esperar y reintentar |

### Scripts de DiagnÃ³stico
```bash
# Verificar conectividad LLM
python scripts/test_llm_connection.py

# Probar API completa
python scripts/test_api_manual.py

# Tests automatizados
make test-real  # Con LLM real
make test       # Solo mocks
```

## ğŸ¯ PrÃ³ximos Pasos

### ğŸ‘¶ **Si eres nuevo:**
1. **[Quick Start](QUICK_START.md)** - Tu primera integraciÃ³n en 5 minutos
2. Probar los ejemplos bÃ¡sicos
3. Experimentar con diferentes parÃ¡metros

### ğŸ§‘â€ğŸ’» **Si eres desarrollador:**
1. **[GuÃ­a de IntegraciÃ³n](INTEGRATION_GUIDE.md)** - DocumentaciÃ³n completa
2. **[Ejemplos de API](API_EXAMPLES.md)** - Casos de uso especÃ­ficos
3. Implementar manejo de errores robusto

### ğŸ—ï¸ **Si vas a producciÃ³n:**
1. Configurar variables de entorno
2. Implementar caching y connection pooling
3. Configurar monitoreo y alertas
4. Leer secciÃ³n de optimizaciÃ³n

## ğŸ¤ Soporte

- **DocumentaciÃ³n API**: `http://localhost:8000/docs`
- **Health Check**: `http://localhost:8000/health`
- **Scripts de Testing**: `scripts/`
- **ConfiguraciÃ³n**: `.env.example`

---

**Â¡Comienza tu integraciÃ³n con LLM ahora!** ğŸš€

**[ğŸ‘‰ Quick Start - Â¡Empezar en 5 minutos!](QUICK_START.md)**