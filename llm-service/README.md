# Servicio LLM con FastAPI y LM Studio Client

Este proyecto implementa un servicio RESTful utilizando FastAPI para interactuar con un modelo de lenguaje grande (LLM) alojado en LM Studio a travÃ©s de la librerÃ­a `lmstudio-client`.

## ğŸš€ CaracterÃ­sticas

- **API REST robusta** con FastAPI
- **ValidaciÃ³n de entrada** con Pydantic
- **Manejo de errores** granular y especÃ­fico
- **Logging estructurado** con correlation IDs
- **Health checks** para monitoreo
- **Rate limiting** y headers de seguridad
- **Reintentos automÃ¡ticos** con backoff exponencial
- **Tests comprehensivos** (unitarios, integraciÃ³n, y conexiÃ³n real)

## ğŸ“‹ Requisitos

### Software Necesario
- Python 3.8+
- [LM Studio](https://lmstudio.ai/) instalado y configurado
- Un modelo de lenguaje cargado en LM Studio

### Dependencias
- FastAPI para la API REST
- Pydantic para validaciÃ³n de datos
- lmstudio-client para conexiÃ³n con LM Studio
- Uvicorn como servidor ASGI

## ğŸ› ï¸ InstalaciÃ³n

### 1. Clonar y configurar el proyecto

```bash
git clone <repo-url>
cd llm-service

# Instalar dependencias de producciÃ³n
pip install -r requirements.txt

# O instalar dependencias de desarrollo (incluye herramientas de testing)
pip install -r requirements-dev.txt
```

### 2. Configurar variables de entorno

```bash
# Copiar template de configuraciÃ³n
cp .env.example .env

# Editar .env con tu configuraciÃ³n
nano .env
```

### 3. Configurar LM Studio

1. Abrir LM Studio
2. Cargar un modelo de lenguaje
3. Iniciar el servidor local (normalmente en `localhost:1234`)
4. Verificar que el modelo estÃ© activo y respondiendo

## ğŸš€ Uso

### Iniciar el Servicio

```bash
# Modo desarrollo (con auto-reload)
make run-dev
# O: uvicorn app.main:app --reload

# Modo producciÃ³n
make run
# O: uvicorn app.main:app --host 0.0.0.0 --port 8000
```

El servicio estarÃ¡ disponible en:
- **API**: http://127.0.0.1:8000
- **DocumentaciÃ³n**: http://127.0.0.1:8000/docs
- **Health Check**: http://127.0.0.1:8000/health

### Endpoints Disponibles

#### `GET /`
InformaciÃ³n bÃ¡sica del servicio

#### `GET /health`
Estado de salud del servicio y conectividad con LM Studio

#### `POST /llm/message`
Enviar mensaje al LLM

**Ejemplo de peticiÃ³n:**
```json
{
  "model": "tu-modelo-llm",
  "messages": [
    {
      "role": "user",
      "content": "Â¡Hola! Â¿CÃ³mo estÃ¡s?"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 150,
  "top_p": 0.9
}
```

**Ejemplo de respuesta:**
```json
{
  "response": "Â¡Hola! Estoy muy bien, gracias por preguntar. Â¿En quÃ© puedo ayudarte hoy?",
  "model": "tu-modelo-llm",
  "tokens_used": 25,
  "processing_time": 1.42,
  "correlation_id": "abc-123-def"
}
```

## ğŸ§ª Testing

### Tests RÃ¡pidos (Solo Mocks)

```bash
# Todos los tests sin conexiÃ³n real
make test

# Solo tests unitarios
make test-unit

# Tests de integraciÃ³n (sin LLM real)
make test-integration

# Tests con reporte de cobertura
make test-cov
```

### Tests con ConexiÃ³n Real a LM Studio

âš ï¸ **Importante**: Estos tests requieren que LM Studio estÃ© ejecutÃ¡ndose con un modelo cargado.

```bash
# Verificar conectividad con LM Studio
make test-connection

# Tests completos con LLM real
make test-real

# Probar API manualmente (requiere servicio corriendo)
make test-api
```

### Scripts de ValidaciÃ³n Manual

#### Verificar ConexiÃ³n LLM

```bash
# Script interactivo para probar conexiÃ³n
python scripts/test_llm_connection.py
```

Este script:
- âœ… Verifica que LM Studio estÃ© corriendo
- âœ… Prueba la inicializaciÃ³n del servicio
- âœ… EnvÃ­a mensajes de prueba
- âœ… Valida diferentes parÃ¡metros
- âœ… Realiza health checks

#### Probar API HTTP

```bash
# Primero iniciar el servicio
make run-dev

# En otra terminal, probar la API
python scripts/test_api_manual.py
```

Este script:
- ğŸŒ Verifica que la API estÃ© respondiendo
- ğŸ¥ Prueba el endpoint de health
- ğŸ’¬ EnvÃ­a mensajes simples y conversaciones
- âœ… Valida manejo de errores
- ğŸ”„ Prueba peticiones concurrentes

## ğŸ“Š ConfiguraciÃ³n de Testing

### Variables de Entorno para Tests

```bash
# Saltar tests que requieren LM Studio real
export SKIP_REAL_LLM_TESTS=true

# Ejecutar solo tests de LLM real
export ONLY_REAL_LLM_TESTS=true

# Saltar tests de rendimiento
export SKIP_PERFORMANCE_TESTS=true

# Timeout para tests largos
export TEST_TIMEOUT=30
```

### Marcadores de Tests

- `real_llm`: Tests que requieren LM Studio real
- `mock_llm`: Tests que usan mocks
- `integration`: Tests de integraciÃ³n
- `performance`: Tests de rendimiento
- `slow`: Tests que tardan tiempo

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

| Variable | DescripciÃ³n | Valor por Defecto |
|----------|-------------|-------------------|
| `LLM_SERVICE_LM_STUDIO_HOST` | Host de LM Studio | `localhost` |
| `LLM_SERVICE_LM_STUDIO_PORT` | Puerto de LM Studio | `1234` |
| `LLM_SERVICE_LM_STUDIO_TIMEOUT` | Timeout en segundos | `30` |
| `LLM_SERVICE_MAX_REQUEST_SIZE` | TamaÃ±o mÃ¡ximo de peticiÃ³n | `10000` |
| `LLM_SERVICE_RATE_LIMIT_REQUESTS` | Peticiones por minuto | `100` |
| `LLM_SERVICE_LOG_LEVEL` | Nivel de logging | `INFO` |
| `LLM_SERVICE_DEBUG` | Modo debug | `false` |

### ConfiguraciÃ³n de ProducciÃ³n

Para producciÃ³n, asegÃºrate de:

1. **Configurar CORS apropiadamente**:
   ```bash
   LLM_SERVICE_ALLOWED_ORIGINS=["https://tu-dominio.com"]
   ```

2. **Establecer hosts de confianza**:
   ```bash
   LLM_SERVICE_TRUSTED_HOSTS=["tu-dominio.com"]
   ```

3. **Configurar logging JSON**:
   ```bash
   LLM_SERVICE_LOG_FORMAT=json
   LLM_SERVICE_LOG_LEVEL=INFO
   ```

4. **Desactivar debug**:
   ```bash
   LLM_SERVICE_DEBUG=false
   ```

## ğŸ” Monitoreo y Logs

### Health Checks

El endpoint `/health` proporciona:
- Estado del servicio
- Conectividad con LM Studio
- Tiempo activo del servicio
- VersiÃ³n de la aplicaciÃ³n

### Logging Estructurado

Todos los logs incluyen:
- Timestamp ISO
- Nivel de log
- Correlation ID para trackear peticiones
- InformaciÃ³n contextual (mÃ©todo, URL, tiempo de procesamiento)

### Headers de Respuesta

Cada respuesta incluye:
- `X-Correlation-ID`: ID Ãºnico para trackear la peticiÃ³n
- `X-Process-Time`: Tiempo de procesamiento en segundos
- Headers de seguridad estÃ¡ndar

## ğŸ› ï¸ Desarrollo

### Calidad de CÃ³digo

```bash
# Verificar estilo de cÃ³digo
make lint

# Formatear cÃ³digo automÃ¡ticamente
make format

# Limpiar archivos temporales
make clean
```

### Estructura del Proyecto

```
llm-service/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # AplicaciÃ³n FastAPI principal
â”‚   â”œâ”€â”€ models.py            # Modelos Pydantic
â”‚   â”œâ”€â”€ config.py            # ConfiguraciÃ³n
â”‚   â”œâ”€â”€ exceptions.py        # Excepciones personalizadas
â”‚   â”œâ”€â”€ middleware.py        # Middleware personalizado
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ llm_service.py   # LÃ³gica de negocio LLM
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_*.py           # Tests unitarios e integraciÃ³n
â”‚   â””â”€â”€ test_real_llm_connection.py  # Tests con LLM real
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ test_llm_connection.py      # ValidaciÃ³n manual LLM
â”‚   â””â”€â”€ test_api_manual.py          # ValidaciÃ³n manual API
â”œâ”€â”€ requirements.txt         # Dependencias producciÃ³n
â”œâ”€â”€ requirements-dev.txt     # Dependencias desarrollo
â”œâ”€â”€ pytest.ini             # ConfiguraciÃ³n de pytest
â”œâ”€â”€ Makefile               # Comandos Ãºtiles
â””â”€â”€ .env.example          # Template de configuraciÃ³n
```

## ğŸš¨ Troubleshooting

### Error: "No se pudo conectar con LM Studio"

1. Verificar que LM Studio estÃ© ejecutÃ¡ndose
2. Confirmar que el puerto sea el correcto (por defecto 1234)
3. Asegurarse de que hay un modelo cargado
4. Revisar la configuraciÃ³n en `.env`

### Error: "LLM request timeout"

1. Incrementar `LLM_SERVICE_LM_STUDIO_TIMEOUT`
2. Verificar que el modelo no estÃ© sobrecargado
3. Reducir `max_tokens` en las peticiones

### Tests fallando

1. Para tests mock: `make test-mock`
2. Para verificar LM Studio: `make test-connection`
3. Para debugging: `pytest -v -s`

### Logs no aparecen

1. Configurar `LLM_SERVICE_LOG_LEVEL=DEBUG`
2. Para formato legible: `LLM_SERVICE_LOG_FORMAT=text`

## ğŸ“„ Licencia

[Especificar licencia del proyecto]

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crear branch para feature (`git checkout -b feature/AmazingFeature`)
3. Commit cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push al branch (`git push origin feature/AmazingFeature`)
5. Abrir Pull Request

---

**Â¡Tu microservicio LLM estÃ¡ listo para producciÃ³n!** ğŸš€